{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from natsort import natsorted \n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: natsort in /Users/arushiverma/anaconda3/lib/python3.11/site-packages (8.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each folder name (string) to an integer label\n",
    "labels_map = {\n",
    "    \"Baroque\": 0,\n",
    "    \"Classical\": 1,\n",
    "    \"Romantic\": 2\n",
    "}\n",
    "num_classes = len(labels_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (7674, 128, 216, 1)\n",
      "y shape: (7674, 3)\n"
     ]
    }
   ],
   "source": [
    "import librosa.display\n",
    "\n",
    "data_dir = \"resampled_dataset\"  # Adjust to your dataset path\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for label_str, label_idx in labels_map.items():\n",
    "    folder_path = os.path.join(data_dir, label_str)\n",
    "    \n",
    "    # Iterate over all audio files in the current folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # 1. Load audio\n",
    "            signal, sr = librosa.load(file_path, sr=22050)\n",
    "            \n",
    "            # 2. Compute mel-spectrogram\n",
    "            S = librosa.feature.melspectrogram(y=signal, sr=sr, n_mels=128, fmax=sr/2)\n",
    "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "            \n",
    "            # 4. Append to X, y\n",
    "            X.append(S_dB)\n",
    "            y.append(label_idx)\n",
    "\n",
    "# Convert X and y to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# If you want a channel dimension for Keras Conv2D, reshape X to (samples, freq, time, 1)\n",
    "X = X[..., np.newaxis]  # adds a channel dimension\n",
    "\n",
    "# One-hot encode labels for categorical crossentropy\n",
    "y = to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (5371, 128, 216, 1) (5371, 3)\n",
      "Val size: (1151, 128, 216, 1) (1151, 3)\n",
      "Test size: (1152, 128, 216, 1) (1152, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape, y_train.shape)\n",
    "print(\"Val size:\", X_val.shape, y_val.shape)\n",
    "print(\"Test size:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arushiverma/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 268ms/step - accuracy: 0.3869 - loss: 4.5214 - val_accuracy: 0.3814 - val_loss: 1.0903\n",
      "Epoch 2/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 322ms/step - accuracy: 0.3681 - loss: 1.0940 - val_accuracy: 0.3814 - val_loss: 1.0906\n",
      "Epoch 3/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 304ms/step - accuracy: 0.3975 - loss: 1.0858 - val_accuracy: 0.3814 - val_loss: 1.0907\n",
      "Epoch 4/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 319ms/step - accuracy: 0.3859 - loss: 1.0899 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 5/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 271ms/step - accuracy: 0.3696 - loss: 1.0939 - val_accuracy: 0.3814 - val_loss: 1.0905\n",
      "Epoch 6/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 61ms/step - accuracy: 0.3767 - loss: 1.0935 - val_accuracy: 0.3814 - val_loss: 1.0906\n",
      "Epoch 7/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 62ms/step - accuracy: 0.3799 - loss: 1.0909 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 8/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 64ms/step - accuracy: 0.3846 - loss: 1.0908 - val_accuracy: 0.3814 - val_loss: 1.0905\n",
      "Epoch 9/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 62ms/step - accuracy: 0.3803 - loss: 1.0914 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 10/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 63ms/step - accuracy: 0.3686 - loss: 1.0937 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 11/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 63ms/step - accuracy: 0.3787 - loss: 1.0928 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 12/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 63ms/step - accuracy: 0.3844 - loss: 1.0897 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 13/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 65ms/step - accuracy: 0.3811 - loss: 1.0896 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 14/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 64ms/step - accuracy: 0.3796 - loss: 1.0912 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 15/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 66ms/step - accuracy: 0.3793 - loss: 1.0916 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 16/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 64ms/step - accuracy: 0.3812 - loss: 1.0922 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 17/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 65ms/step - accuracy: 0.3834 - loss: 1.0917 - val_accuracy: 0.3814 - val_loss: 1.0905\n",
      "Epoch 18/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 66ms/step - accuracy: 0.3757 - loss: 1.0929 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 19/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 68ms/step - accuracy: 0.3791 - loss: 1.0921 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "Epoch 20/20\n",
      "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 69ms/step - accuracy: 0.3802 - loss: 1.0911 - val_accuracy: 0.3814 - val_loss: 1.0904\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.3721 - loss: 1.0890\n",
      "Test Accuracy: 0.3819444477558136\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "input_shape = (128, X_train.shape[2], 1)  # (freq_bins, time_frames, channels)\n",
    "model = build_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
