{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from natsort import natsorted \n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all composers with more than 10 recordings in the dataset(from excel analaysis)\n",
    "composer_list=['Beethoven', 'Bach', 'Schubert', 'Handel', 'Brahms', 'Schumann', 'Mozart', 'Dvorak', 'Vivaldi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: resampled_dataset\n",
      "Found subfolders: ['Classical', 'Baroque', 'Modern', 'Romantic']\n",
      "Extracting features from audio files...\n",
      "Processing folder: Classical\n",
      "Processing folder: Baroque\n",
      "Processing folder: Modern\n",
      "Processing folder: Romantic\n",
      "Extracted features from 9530 audio files.\n",
      "Features array shape: (9530, 128, 646, 1)\n",
      "Labels array shape: (9530,)\n",
      "Training set shape: (6861, 128, 646, 1)\n",
      "Validation set shape: (763, 128, 646, 1)\n",
      "Test set shape: (1906, 128, 646, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arushiverma/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 176ms/step - accuracy: 0.3292 - loss: 21.0691 - val_accuracy: 0.3866 - val_loss: 1.3139\n",
      "Epoch 2/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 168ms/step - accuracy: 0.3455 - loss: 1.3538 - val_accuracy: 0.3984 - val_loss: 1.2908\n",
      "Epoch 3/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 168ms/step - accuracy: 0.3833 - loss: 1.3120 - val_accuracy: 0.4128 - val_loss: 1.2586\n",
      "Epoch 4/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 180ms/step - accuracy: 0.4137 - loss: 1.2748 - val_accuracy: 0.4102 - val_loss: 1.2875\n",
      "Epoch 5/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 173ms/step - accuracy: 0.4218 - loss: 1.2477 - val_accuracy: 0.4194 - val_loss: 1.3010\n",
      "Epoch 6/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 177ms/step - accuracy: 0.4492 - loss: 1.2199 - val_accuracy: 0.4522 - val_loss: 1.2207\n",
      "Epoch 7/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 176ms/step - accuracy: 0.4705 - loss: 1.1958 - val_accuracy: 0.4443 - val_loss: 1.2533\n",
      "Epoch 8/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 171ms/step - accuracy: 0.4716 - loss: 1.1760 - val_accuracy: 0.4862 - val_loss: 1.1763\n",
      "Epoch 9/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 171ms/step - accuracy: 0.5161 - loss: 1.1031 - val_accuracy: 0.4875 - val_loss: 1.2017\n",
      "Epoch 10/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 178ms/step - accuracy: 0.4688 - loss: 1.2183 - val_accuracy: 0.3067 - val_loss: 1.3720\n",
      "Epoch 11/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 180ms/step - accuracy: 0.3001 - loss: 1.3737 - val_accuracy: 0.3067 - val_loss: 1.3714\n",
      "Epoch 12/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 184ms/step - accuracy: 0.3055 - loss: 1.3726 - val_accuracy: 0.3067 - val_loss: 1.3715\n",
      "Epoch 13/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 173ms/step - accuracy: 0.3135 - loss: 1.3680 - val_accuracy: 0.3067 - val_loss: 1.3716\n",
      "Epoch 14/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 179ms/step - accuracy: 0.3083 - loss: 1.3719 - val_accuracy: 0.3067 - val_loss: 1.3715\n",
      "Epoch 15/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 185ms/step - accuracy: 0.3057 - loss: 1.3718 - val_accuracy: 0.3067 - val_loss: 1.3714\n",
      "Epoch 16/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 172ms/step - accuracy: 0.3163 - loss: 1.3674 - val_accuracy: 0.3067 - val_loss: 1.3714\n",
      "Epoch 17/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 169ms/step - accuracy: 0.2966 - loss: 1.3738 - val_accuracy: 0.3067 - val_loss: 1.3716\n",
      "Epoch 18/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 173ms/step - accuracy: 0.3134 - loss: 1.3707 - val_accuracy: 0.3067 - val_loss: 1.3714\n",
      "Epoch 19/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 174ms/step - accuracy: 0.3071 - loss: 1.3726 - val_accuracy: 0.3067 - val_loss: 1.3714\n",
      "Epoch 20/20\n",
      "\u001b[1m429/429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 186ms/step - accuracy: 0.3054 - loss: 1.3706 - val_accuracy: 0.3067 - val_loss: 1.3714\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - accuracy: 0.2980 - loss: 1.3724\n",
      "Test Accuracy: 0.30797481536865234\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import config1  # This module should define your configuration parameters\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def get_subdirectories(a_dir):\n",
    "    \"\"\"\n",
    "    Returns a list of subfolder names inside the given directory.\n",
    "    Each subfolder typically represents a class (e.g., 'Baroque', 'Classical', 'Romantic').\n",
    "    \"\"\"\n",
    "    return [name for name in os.listdir(a_dir) if os.path.isdir(os.path.join(a_dir, name))]\n",
    "\n",
    "def get_audios_path(dataset_dir, folder_name):\n",
    "    \"\"\"\n",
    "    Returns a list of audio file paths from a given subfolder.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_dir: The base dataset directory.\n",
    "    - folder_name: The name of the subfolder.\n",
    "    \n",
    "    Returns:\n",
    "    - List of full paths to audio files (e.g., wav or mp3) within that folder.\n",
    "    \"\"\"\n",
    "    folder_path = os.path.join(dataset_dir, folder_name)\n",
    "    audio_paths = librosa.util.find_files(folder_path, ext=['wav', 'mp3'])\n",
    "    return audio_paths\n",
    "\n",
    "def extract_features(audio_path, samp_rate, frame_size, hop_size, n_mels=128, fixed_frames=646):\n",
    "    \"\"\"\n",
    "    Loads an audio file and computes its log-mel spectrogram, then pads or crops\n",
    "    it to ensure the time dimension is fixed.\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_path: Full path to the audio file.\n",
    "    - samp_rate: Sampling rate to use when loading audio.\n",
    "    - frame_size: FFT window size.\n",
    "    - hop_size: Hop (stride) size for the FFT.\n",
    "    - n_mels: Number of mel bins (default 128).\n",
    "    - fixed_frames: The desired fixed number of time frames.\n",
    "    \n",
    "    Returns:\n",
    "    - log_S: A NumPy array (shape: (n_mels, fixed_frames)) containing the log-mel spectrogram.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the audio file (entire file; you could specify duration if needed)\n",
    "        y, sr = librosa.load(audio_path, sr=samp_rate)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Compute the mel-spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=frame_size, hop_length=hop_size, n_mels=n_mels)\n",
    "    # Convert to log scale (dB)\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "    \n",
    "    # Ensure a fixed time dimension by padding or cropping\n",
    "    if log_S.shape[1] < fixed_frames:\n",
    "        pad_width = fixed_frames - log_S.shape[1]\n",
    "        log_S = np.pad(log_S, ((0, 0), (0, pad_width)), mode='constant', constant_values=log_S.min())\n",
    "    else:\n",
    "        log_S = log_S[:, :fixed_frames]\n",
    "    \n",
    "    return log_S\n",
    "\n",
    "def main():\n",
    "    # Retrieve configuration parameters from config1.CreateDataset\n",
    "    samp_rate = config1.CreateDataset.SAMPLING_RATE\n",
    "    frame_size = config1.CreateDataset.FRAME_SIZE\n",
    "    hop_size = config1.CreateDataset.HOP_SIZE\n",
    "    # You can define FIXED_FRAMES in your config or set it here\n",
    "    fixed_frames = getattr(config1.CreateDataset, 'FIXED_FRAMES', 646)\n",
    "    dataset_dir = 'resampled_dataset'\n",
    "\n",
    "    print(\"Dataset directory:\", dataset_dir)\n",
    "    \n",
    "    # Get a list of subfolders (each corresponding to a class like 'Baroque', etc.)\n",
    "    sub_folders = get_subdirectories(dataset_dir)\n",
    "    print(\"Found subfolders:\", sub_folders)\n",
    "\n",
    "    # Lists to hold features and labels\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    print(\"Extracting features from audio files...\")\n",
    "    # Process each subfolder (class) in the dataset\n",
    "    for sub_folder in sub_folders:\n",
    "        print(f\"Processing folder: {sub_folder}\")\n",
    "        # Get all audio file paths from this subfolder\n",
    "        audio_paths = get_audios_path(dataset_dir, sub_folder)\n",
    "        for audio_path in audio_paths:\n",
    "            feat = extract_features(audio_path, samp_rate, frame_size, hop_size, n_mels=128, fixed_frames=fixed_frames)\n",
    "            if feat is not None:\n",
    "                features_list.append(feat)\n",
    "                labels_list.append(sub_folder)\n",
    "    \n",
    "    # Convert lists to numpy arrays; now each feature will have the same shape (128, fixed_frames)\n",
    "    features_array = np.array(features_list)\n",
    "    labels_array = np.array(labels_list)\n",
    "    \n",
    "    # Expand dims to add a channel dimension for the CNN (expected shape: (n_samples, freq_bins, fixed_frames, 1))\n",
    "    X = np.expand_dims(features_array, axis=-1)\n",
    "    y = labels_array  # Still as strings for now\n",
    "\n",
    "    print(\"Extracted features from\", len(features_list), \"audio files.\")\n",
    "    print(\"Features array shape:\", X.shape)\n",
    "    print(\"Labels array shape:\", y.shape)\n",
    "    \n",
    "    # Split the dataset (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Further split training set into training and validation (e.g., 10% of training for validation)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(\"Training set shape:\", X_train.shape)\n",
    "    print(\"Validation set shape:\", X_val.shape)\n",
    "    print(\"Test set shape:\", X_test.shape)\n",
    "    \n",
    "    # Determine number of classes and map string labels to integer indices\n",
    "    class_names = np.unique(y)\n",
    "    num_classes = len(class_names)\n",
    "    class_to_int = {name: i for i, name in enumerate(class_names)}\n",
    "    \n",
    "    y_train_int = np.array([class_to_int[label] for label in y_train])\n",
    "    y_val_int = np.array([class_to_int[label] for label in y_val])\n",
    "    y_test_int = np.array([class_to_int[label] for label in y_test])\n",
    "    \n",
    "    # One-hot encode the labels\n",
    "    y_train_cat = to_categorical(y_train_int, num_classes=num_classes)\n",
    "    y_val_cat = to_categorical(y_val_int, num_classes=num_classes)\n",
    "    y_test_cat = to_categorical(y_test_int, num_classes=num_classes)\n",
    "    \n",
    "    def build_cnn_model(input_shape, num_classes):\n",
    "        model = models.Sequential()\n",
    "    \n",
    "        model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "        model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(128, activation='relu'))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    # Define input shape based on your features.\n",
    "    # Here, we have 128 mel bins, fixed_frames time steps, and 1 channel.\n",
    "    input_shape = (128, fixed_frames, 1)\n",
    "    model = build_cnn_model(input_shape, num_classes)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs=20,\n",
    "        batch_size=16\n",
    "    )\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_cat)\n",
    "    print(\"Test Accuracy:\", test_acc)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
