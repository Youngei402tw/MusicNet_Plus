{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from natsort import natsorted \n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all composers with more than 10 recordings in the dataset(from excel analaysis)\n",
    "composer_list=['Beethoven', 'Bach', 'Schubert', 'Mozart', 'Brahms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "composer\n",
       "Beethoven    157\n",
       "Bach          67\n",
       "Schubert      30\n",
       "Mozart        24\n",
       "Brahms        24\n",
       "Cambini        9\n",
       "Dvorak         8\n",
       "Faure          4\n",
       "Ravel          4\n",
       "Haydn          3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"musicnet_original_metadata.csv\", encoding=\"ISO-8859-1\")\n",
    "#row = df.loc[df['id'] == 1727]\n",
    "#print(row)\n",
    "df['composer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from audios...\n",
      ".....Working in folder: test_labels\n",
      ".....Working in folder: test_data\n",
      ".....Working in folder: train_data\n",
      ".....Working in folder: train_labels\n",
      "Extracted features from 302 audio files.\n",
      "Features array shape: (302, 128, 130, 1)\n",
      "Labels array shape: (302,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arushiverma/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.3210 - loss: 7.0524 - val_accuracy: 0.5200 - val_loss: 1.2960\n",
      "Epoch 2/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5455 - loss: 1.2597 - val_accuracy: 0.5200 - val_loss: 1.2944\n",
      "Epoch 3/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5316 - loss: 1.1906 - val_accuracy: 0.5600 - val_loss: 1.1936\n",
      "Epoch 4/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5652 - loss: 1.0538 - val_accuracy: 0.5600 - val_loss: 1.0840\n",
      "Epoch 5/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.6005 - loss: 0.9268 - val_accuracy: 0.6000 - val_loss: 1.1256\n",
      "Epoch 6/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7647 - loss: 0.7930 - val_accuracy: 0.6000 - val_loss: 1.0282\n",
      "Epoch 7/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8043 - loss: 0.7023 - val_accuracy: 0.5600 - val_loss: 1.1249\n",
      "Epoch 8/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.7873 - loss: 0.5781 - val_accuracy: 0.5600 - val_loss: 1.1869\n",
      "Epoch 9/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8331 - loss: 0.4813 - val_accuracy: 0.7200 - val_loss: 1.2143\n",
      "Epoch 10/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8420 - loss: 0.4209 - val_accuracy: 0.7200 - val_loss: 1.3856\n",
      "Epoch 11/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8959 - loss: 0.3550 - val_accuracy: 0.7200 - val_loss: 1.2981\n",
      "Epoch 12/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9477 - loss: 0.2585 - val_accuracy: 0.7600 - val_loss: 1.9627\n",
      "Epoch 13/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9184 - loss: 0.2209 - val_accuracy: 0.7600 - val_loss: 1.3844\n",
      "Epoch 14/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9313 - loss: 0.2340 - val_accuracy: 0.6800 - val_loss: 1.5163\n",
      "Epoch 15/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9774 - loss: 0.1274 - val_accuracy: 0.7200 - val_loss: 1.8552\n",
      "Epoch 16/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9746 - loss: 0.1378 - val_accuracy: 0.6800 - val_loss: 2.2376\n",
      "Epoch 17/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9875 - loss: 0.0868 - val_accuracy: 0.6000 - val_loss: 2.2801\n",
      "Epoch 18/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9699 - loss: 0.1159 - val_accuracy: 0.6000 - val_loss: 1.8637\n",
      "Epoch 19/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9728 - loss: 0.0942 - val_accuracy: 0.6000 - val_loss: 2.0124\n",
      "Epoch 20/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9693 - loss: 0.1163 - val_accuracy: 0.6800 - val_loss: 1.6486\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5169 - loss: 2.2454\n",
      "Test Accuracy: 0.5409836173057556\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "import config1\n",
    "\n",
    "# Your chosen composers\n",
    "composer_list = ['Beethoven', 'Bach', 'Schubert', 'Mozart', 'Brahms']\n",
    "\n",
    "# Load metadata\n",
    "df = pd.read_csv(\"musicnet_original_metadata.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Helper function to get subdirectories\n",
    "def get_subdirectories(directory):\n",
    "    return [f for f in os.listdir(directory) if os.path.isdir(os.path.join(directory, f))]\n",
    "\n",
    "# Helper function to get audio file paths\n",
    "def get_audios_path(base_dir, sub_folder, samp_rate):\n",
    "    folder_path = os.path.join(base_dir, sub_folder)\n",
    "    return [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
    "\n",
    "# Dummy placeholder - replace with your own feature extraction logic\n",
    "def extract_features(file_path, sr, frame_size, hop_size):\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=frame_size, hop_length=hop_size, n_mels=128)\n",
    "    log_mel = librosa.power_to_db(mel)\n",
    "    \n",
    "    # Fix time frames to a constant length (pad or cut)\n",
    "    fixed_frames = 130\n",
    "    if log_mel.shape[1] < fixed_frames:\n",
    "        pad_width = fixed_frames - log_mel.shape[1]\n",
    "        log_mel = np.pad(log_mel, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        log_mel = log_mel[:, :fixed_frames]\n",
    "\n",
    "    return log_mel\n",
    "\n",
    "def main():\n",
    "    samp_rate = config1.CreateDataset.SAMPLING_RATE\n",
    "    frame_size = config1.CreateDataset.FRAME_SIZE\n",
    "    hop_size = config1.CreateDataset.HOP_SIZE\n",
    "    dataset_dir = \"musicnet/musicnet\"\n",
    "\n",
    "    sub_folders = get_subdirectories(dataset_dir)\n",
    "\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    print(\"Extracting features from audios...\")\n",
    "    for sub_folder in sub_folders:\n",
    "        print(\".....Working in folder:\", sub_folder)\n",
    "        audios_path = get_audios_path(dataset_dir, sub_folder, samp_rate)\n",
    "        \n",
    "        for audio_path in audios_path:\n",
    "            audio_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "            row = df.loc[df['id'] == int(audio_name)]\n",
    "\n",
    "            if row.empty:\n",
    "                continue\n",
    "\n",
    "            label = row.iloc[0]['composer']\n",
    "            if label in composer_list:\n",
    "                features = extract_features(audio_path, samp_rate, frame_size, hop_size)\n",
    "                features_list.append(features)\n",
    "                labels_list.append(label)\n",
    "\n",
    "    features_array = np.array(features_list)\n",
    "    labels_array = np.array(labels_list)\n",
    "\n",
    "    X = np.expand_dims(features_array, axis=-1)\n",
    "    y = labels_array\n",
    "\n",
    "    print(\"Extracted features from\", len(features_list), \"audio files.\")\n",
    "    print(\"Features array shape:\", X.shape)\n",
    "    print(\"Labels array shape:\", y.shape)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, stratify=y_train)\n",
    "\n",
    "    # Encode labels\n",
    "    class_names = np.unique(y)\n",
    "    num_classes = len(class_names)\n",
    "    class_to_int = {name: i for i, name in enumerate(class_names)}\n",
    "\n",
    "    y_train_int = np.array([class_to_int[label] for label in y_train])\n",
    "    y_val_int = np.array([class_to_int[label] for label in y_val])\n",
    "    y_test_int = np.array([class_to_int[label] for label in y_test])\n",
    "\n",
    "    y_train_cat = to_categorical(y_train_int, num_classes=num_classes)\n",
    "    y_val_cat = to_categorical(y_val_int, num_classes=num_classes)\n",
    "    y_test_cat = to_categorical(y_test_int, num_classes=num_classes)\n",
    "\n",
    "    def build_cnn_model(input_shape, num_classes):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(128, activation='relu'))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    input_shape = X_train.shape[1:]  # Should be (128, fixed_frames, 1)\n",
    "    model = build_cnn_model(input_shape, num_classes)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs=20,\n",
    "        batch_size=16\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_cat)\n",
    "    print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
